{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a simple model on the MNIST dataset\n",
    "\n",
    "In this notebook we are going to get familiar with using a deep learning library like Tensorflow to train a simple neural network. The network will be trained on the [MNIST dataset](http://yann.lecun.com/exdb/mnist/) which contains small images of handwritten numerical digits. By the end of this training, the model should be able to accurately classify images with numerical digits.\n",
    "\n",
    "Training a network on the MNIST dataset has become the 'hello world' of machine learning. \n",
    "\n",
    "This notebook is based on a notebook originally by [fchollet](https://twitter.com/fchollet) - the original creator of Keras.\n",
    "\n",
    "### Installing TensorFlow / Keras\n",
    "\n",
    "If you haven't installed TensorFlow and Keras, follow the instructions in [python setup instructions](https://github.com/colormotor/DMLAP/tree/main/python%20setup%20instructions).\n",
    "\n",
    "### Setting up for Google Colab (Collab only!)\n",
    "On Google Colab you will need to execute the following cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyyaml h5py  # Required to save models in HDF5 format\n",
    "!wget https://raw.githubusercontent.com/colormotor/DMLAP/main/python/images.zip # Get required image files\n",
    "!unzip images.zip"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But to use the model created by this notebook in another, you will need to either manually download/upload the model file, or setup your notebook to mount a Google drive. For the latter option you can find instructions on the web, for example [here](https://towardsdatascience.com/different-ways-to-connect-google-drive-to-a-google-colab-notebook-pt-1-de03433d2f7a)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Importing TensorFlow / Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the dataset\n",
    "\n",
    "The code in the following cell:\n",
    "\n",
    "- defines the number of classes for digits (0-9)\n",
    "\n",
    "- defines the size of the input data (grayscale images 28x28 pixels)\n",
    "\n",
    "- loads the MNIST dataset using the utils functions in Keras\n",
    "\n",
    "- prepares the data for training. When training neural networks it is important the ensure that the size of the dataset is the same as the size of the input of the model, otherwise things will quickly break!\n",
    "\n",
    "- converts class vectors to binary class matrices using the [utils functions in Keras](https://keras.io/api/utils/python_utils/#to_categorical-function)\n",
    "\n",
    "**Things to try out:**\n",
    "\n",
    "- Print the first item of x_train before and after you convert the array from uint8 (8-bit integers) to float32 (32-bit floats) \n",
    "- Print the first 10 items of y_train before and after you convert them to matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# Model / data parameters\n",
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "# Load the data and split it between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "\n",
    "# Make sure images have shape (28, 28, 1)\n",
    "# Expand dimensions from (60000, 28, 28) to (60000, 28, 28, 1). \n",
    "# axis = -1 is adding one more axis to the array\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the model\n",
    "\n",
    "This next section of code is where we build the neural network model, by defining the model in the class `Keras.Sequential` we are defining the *order* in which one layer connects to another, which is how we tell TensorFlow which part of the network connects to the next.\n",
    "\n",
    "This network combines convolutional and pooling layers in the first several layers of the network (see clarifications below), with a dense layer in the end (Dense ~ the Keras term for fully connected). \n",
    "\n",
    "The output of the network is a vector of 10, the same as the number of classes we are classifying. Each one of these units represents a prediction of how likely the network predicts the input digit as being that class. We use the class with the highest confidence as the prediction from the model. \n",
    "\n",
    "**Helpful clarifications:**\n",
    "\n",
    "- Convolutions are used as an extremely powerful tool for pattern recognition.\n",
    "\n",
    "- Kernels are integers or tuples of 2 integers, specifying the height and width of the 2D convolution window, i.e. the window by which we examine a subset of the image.\n",
    "\n",
    "- We can use different kernel sizes when we perform convolutions.\n",
    "\n",
    "- The size of the kernel will determine how big the patterns are that we can detect with our convolution operation.\n",
    "\n",
    "- A deep CNN has convolutional layers stacked on top of each other. Each layer is made up of lots of different feature extractors, responding to different kinds of patterns. The output(s) of one layer becomes the input(s) to the next one.\n",
    "\n",
    "- Max-pooling is used to downsample the output information from one convolutional layer to the next one (by dividing the output matrix into windows and taking the maximum value in the window).\n",
    "\n",
    "- Flattening is converting the data into a 1-dimensional array for inputting it to the next layer. We flatten the output of the convolutional layers to create a single long feature vector.\n",
    "\n",
    "- The Dropout layer randomly sets input units to 0 with a frequency of rate at each step during training time, which helps prevent overfitting.\n",
    "\n",
    "- Th softmax function is used to give us a probability distribution (probabilities that add up to 1) over classes for our input.\n",
    "\n",
    "- 'None' in the output shapes means any batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 13, 13, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 5, 5, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1600)              0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1600)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                16010     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 34,826\n",
      "Trainable params: 34,826\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "\n",
    "This block of code is where we run the training of the model. It is only a few lines of code, because most of what happens in training is handled behind the scenes by TensorFlow. \n",
    "\n",
    "There are two parameters we need to define, the `batch_size` and the number of `epochs`. The batch size defines how many data samples we process at once during training, this helps speed up training if we use a bigger batch size (but is dependent on the size of the memory of our computer). Using a higher batch size generally leads to better results training, as the weights are updated based on the loss of the whole batch, which leads to more stable training than if we were to update the weights after each single example. Training in batches is a form of *regularisation* - something that will come up again and again with different tricks for getting the best performance out of training. \n",
    "\n",
    "The number of `epochs` defines how many iterations we perform over the dataset over training. The more epochs in training we perform, the longer training is going to take, but it often (but not always) leads to better performance.\n",
    "\n",
    "In function call `model.compile` we define the loss function and the optimiser used to update the weights.\n",
    "\n",
    "In function call `model.fit` we actually perform the training of the model.\n",
    "\n",
    "**Things to try out:**\n",
    "\n",
    "- Test the training with different parameters and see how these impact the accuracy of the model:\n",
    "\n",
    "    - change the batch size and the number of epochs\n",
    "    \n",
    "    - search for different [loss functions](https://keras.io/api/losses/) and [optimizers](https://keras.io/api/optimizers/) from the Keras API reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-13 16:29:38.357182: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "213/213 [==============================] - 8s 35ms/step - loss: 0.4822 - accuracy: 0.8585 - val_loss: 0.1074 - val_accuracy: 0.9725\n",
      "Epoch 2/10\n",
      "213/213 [==============================] - 8s 35ms/step - loss: 0.1412 - accuracy: 0.9571 - val_loss: 0.0730 - val_accuracy: 0.9800\n",
      "Epoch 3/10\n",
      "213/213 [==============================] - 8s 35ms/step - loss: 0.1048 - accuracy: 0.9677 - val_loss: 0.0595 - val_accuracy: 0.9842\n",
      "Epoch 4/10\n",
      "213/213 [==============================] - 7s 34ms/step - loss: 0.0876 - accuracy: 0.9735 - val_loss: 0.0496 - val_accuracy: 0.9873\n",
      "Epoch 5/10\n",
      "213/213 [==============================] - 7s 33ms/step - loss: 0.0769 - accuracy: 0.9768 - val_loss: 0.0452 - val_accuracy: 0.9890\n",
      "Epoch 6/10\n",
      "213/213 [==============================] - 7s 33ms/step - loss: 0.0689 - accuracy: 0.9790 - val_loss: 0.0411 - val_accuracy: 0.9900\n",
      "Epoch 7/10\n",
      "213/213 [==============================] - 7s 35ms/step - loss: 0.0618 - accuracy: 0.9809 - val_loss: 0.0387 - val_accuracy: 0.9897\n",
      "Epoch 8/10\n",
      "213/213 [==============================] - 8s 38ms/step - loss: 0.0569 - accuracy: 0.9824 - val_loss: 0.0347 - val_accuracy: 0.9910\n",
      "Epoch 9/10\n",
      "213/213 [==============================] - 8s 39ms/step - loss: 0.0533 - accuracy: 0.9834 - val_loss: 0.0377 - val_accuracy: 0.9898\n",
      "Epoch 10/10\n",
      "213/213 [==============================] - 8s 37ms/step - loss: 0.0494 - accuracy: 0.9844 - val_loss: 0.0348 - val_accuracy: 0.9910\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x166f41600>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 254\n",
    "epochs = 10\n",
    "\n",
    "# Here we are defining the loss function and the optimiser used for training.\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "#Here we call the function that performs training, this will train for the number of epochs we have defined.\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model\n",
    "\n",
    "This next cell is where we evaluate the model. We take our trained model and test it against the test dataset. This will give us an overall accuracy score used to assess the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.0323789119720459\n",
      "Test accuracy: 0.9886999726295471\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model with an actual input image\n",
    "\n",
    "Here we load a test image to see if it classifies it correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28, 1)\n",
      "[[1.6720923e-16 4.2203160e-13 1.3659815e-10 1.0000000e+00 9.1115963e-17\n",
      "  1.8410420e-09 6.5809466e-14 5.6228470e-13 4.0036013e-10 7.1743111e-11]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x15897c520>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = image.load_img('./images/number3.png', target_size=(28, 28), color_mode='grayscale')\n",
    "x = image.img_to_array(img)/255\n",
    "print(x.shape)\n",
    "x = np.expand_dims(x, 0)\n",
    "predictions = model.predict(x, verbose=False)\n",
    "print(predictions)\n",
    "predicted = np.argmax(predictions)\n",
    "plt.figure()\n",
    "plt.title('Predicted number: ' + str(predicted))\n",
    "plt.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model\n",
    "\n",
    "This is where we save the trained model in the models folder to use it in the next notebook for more creative experimentations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./models/basic_mnist.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "1c544d3133b9d8c6f36fca025551af31afa9ef134259e7064ad6be0c15e6401c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
